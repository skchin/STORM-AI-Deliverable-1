\chapter{Executive Summary} \label{cha:alt-executive-summary}

%\vspace{-0.3in}
% \fbox{%
%   \begin{minipage}{\dimexpr\linewidth-2\fboxsep-2\fboxrule\relax}
%     \textbf{Summary of Main Points Bottom Line Up Front (BLUF)}
%     \begin{itemize}
%       \item When we say \emph{trustworthy artificial intelligence} we mean \textbf{trustworthy systems with AI as a subsystem}
%       \item This definition is consistent with the following standards
%         \begin{itemize}
%           \item NIST SP 800-160v1r1, Engineering Secure Trustworthy Systems, \cite{NIST800-160}
%           \item ISO/IEC/IEEE 15288, Systems and software engineering System life cycle processes, \cite{ISO15288}
%         \end{itemize}
%     \end{itemize}
%   \end{minipage}%
% }

\section{Introduction}
Artificial Intelligence (AI) systems are rapidly being integrated into defense and critical infrastructure, yet a fundamental question remains largely unaddressed: how do we systematically ensure these systems are trustworthy for mission-critical operations? As AI capabilities advance and deployment accelerates, the need for rigorous, evidence-based approaches to AI trustworthiness becomes increasingly urgent. This report presents a comprehensive investigation of 54,628 AI research papers published between 2021 and 2025, mapped against established systems engineering frameworks and organizational risk management standards. Our analysis reveals a troubling disconnect between the technical focus of current AI research and the foundational requirements for mission assurance---a gap that poses significant risks to national security and operational effectiveness.

%vspace{-1.5em}
\section{Our Findings}

Only 9,183 (16.8\%) of the 54,628 AI papers examined addressed any aspect of AI trustworthiness, reliability, or security. More critically, virtually all papers related to trustworthiness were concentrated at the lowest level of abstraction---the systems and operations level. The vast majority dealt with AI models, implementations, and algorithm performance. Few or none addressed problem requirements, provided evidence of trustworthiness, or examined concerns at the mission/business process level or organizational governance level.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{STORM-AI Deliverable-1/Figures/Paper-Classification-Results.jpg}
    \caption{\it Results of searching and classifying 54,628 AI papers: fewer than 16.8\% address AI Trustworthiness, Reliability, or Security; few or none of those address organizational or mission requirements. \textbf{Mission Assurance is completely neglected.}}
    \label{fig:heatmapAlt}
\end{figure}

As illustrated in Figure \ref{fig:heatmapAlt}, existing AI research is disconnected from organizational and mission requirements, with no systematic processes for guaranteeing mission-essential functions in systems using AI.

\section{Mission Assurance is Completely Neglected}

\textbf{Mission Assurance}  \cite{DoD3020-40-MA} \emph{is a process to protect or ensure the continued function and resilience of capabilities and assets critical to executing DoD mission-essential functions in any operating environment}\footnote{Definition of Mission Assurance in DoD Directive 3020.40 Mission Assurance, September 2018}. \textbf{Mission-Essential Functions} \cite{DoD3020-26-Continuity-MEF} \emph{are select functions directly related to accomplishing the Department's mission}---failure to perform or sustain these functions would significantly affect the Department of Defense's ability to provide vital services or exercise authority, direction, and control\footnote{Definition of Mission Essential Functions in DoD Directive 3020.26 DoD Continuity Policy, February 2018}.

Our findings show that current AI research lacks systematic processes for mission assurance. \textbf{Developing mission-assurance processes for systems using AI must be a top priority.}

% -- Comments below are at the very end of component files --
% -- pointing to the main file --
% ---- this points LaTeX to STORM-AI-Deliverable-1.tex ---- 
% Local Variables: 
% TeX-master: "STORM-AI-Deliverable-1"
% End:

\section{Recommendations}

Addressing the documented gap in early-lifecycle mission context and requirements requires a disciplined, systems-level approach. We recommend using the System-Theoretic and Technical Operational Risk Management (STORM) methodology \cite{STORM}, with particular emphasis on its STPA-SEC component \cite{STPA-SEC}. STPA-SEC is specifically designed to perform currently missing tasks: generating mission objectives, identifying unacceptable losses and hazards, and defining constraints that must govern system behavior. It aligns policy intent with technical implementation and frames security as a mission-focused issue rather than just a technical challenge.


%\noindent\fbox{\parbox{\dimexpr\textwidth-2\fboxsep-2\fboxrule}{%
%\begin{center}
%\textbf{There is a clear opportunity to strengthen mission context, %early requirements definition, and downstream verification and %validation in AI trustworthiness research.}
%\end{center}
%}}
%\newline
\begin{tcolorbox}
 \textbf{There is a clear opportunity to strengthen mission context, early requirements definition, and downstream verification and validation in AI trustworthiness research.} 
\end{tcolorbox}

Once mission requirements and constraints are established, STORM's Certified Security by Design (CSBD) component \cite{CSBD-IOT} \cite{CSBD-Education} enables formal verification that system designs satisfy them. CSBD provides evidence---not assumptions---that implemented Concepts of Operation (CONOPS) enforce complete mediation, ensuring actions occur only if authorized and authenticated under defined policy. This closes the loop between mission intent, system design, and verification, delivering the demonstrable trustworthiness and mission assurance required by NIST SP 800-160v1r1 \cite{nist_sp800_160v1}.
