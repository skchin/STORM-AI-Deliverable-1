\chapter{Defining Trustworthiness in Artificial Intelligence} \label{defining-trustworthiness-in-ai}

% \fbox{%
%   \begin{minipage}{\dimexpr\linewidth-2\fboxsep-2\fboxrule\relax}
%     \textbf{Summary of Main Points Bottom Line Up Front (BLUF)}
%     \begin{itemize}
%       \item
%     \end{itemize}
%   \end{minipage}%
% }

\section{Importance of Defining the Meaning of Trust}

Systems incorporating Artificial Intelligence are inherently \textit{non-deterministic}, meaning that even after trust has been initially established, the behavior of the system may evolve, necessitating ongoing recalibration of that trust. This expands the dimensions of both trust and trustworthiness beyond context-specific factors to include the \textit{extent and nature of the system's adaptability} to new or emergent behaviors. 

The concept of trust in AI systems remains poorly defined across technical and research communities. This lack of standardization creates confusion and introduces vulnerabilities in system security and assurance practices. In this chapter, we establish a precise, operational definition of trust tailored specifically to the requirements of secure and assured systems.



\subsection{Artificial Intelligence} 

We first define the term Artificial Intelligence (AI), which is loosely used by many different people due to its popularity in everyday life. 
Artificial Intelligence is an algorithm designed to emulate human intelligence by performing tasks that normally require perception, reasoning, learning, communication, or action. It can operate under changing conditions with limited oversight, adapt from data and experience, and shape real or virtual environments through its predictions, recommendations, and decisions. \href{https://media.defense.gov/2019/Feb/12/2002088963/-1/-1/1/SUMMARY-OF-DOD-AI-STRATEGY.PDF\#page=5}{DoD AI Strategy (2018)}, \href{https://www.congress.gov/115/plaws/publ232/PLAW-115publ232.pdf\#page=63}{National Defense Authorization Act (NDAA) 2019}, and the \href{https://www.congress.gov/116/plaws/publ283/PLAW-116publ283.pdf\#page=1138}{National AI Initiative Act of 2020}.


\subsection{Trust}

Trust is a stakeholder's belief about how a system will perform in a specific context. Stakeholders confer trust when they assess that a system is fit for purpose meaning it can perform required tasks to an acceptable level within defined operational contexts.
The decision to grant trust is inherently subjective and relational, shaped by multiple factors: the stakeholder's prior experience with similar systems, the system's reputation, the operational context, and the structure of the decision-making process itself (whether formal or informal). Different stakeholders may reach different trust judgments about the same system based on these varying influences.

\subsection{Trustworthiness}

Trustworthiness is the objective, evidence-based assessment of whether a system meets specified behavioral expectations. It is measured and validated through quantitative evidence (formal verification, empirical testing, performance metrics) or qualitative evidence (expert analysis, structured evaluation). Trustworthiness assessments are always anchored to explicit claims about system capabilities and constraints.

\subsection{Relationship Between Trust and Trustworthiness}

Trust is granted by stakeholders based on their perception of whether a system is fit for purpose, while trustworthiness is an objective measure of system behavior assessed against specific claims. Measuring system performance creates a body of evidence demonstrating that the system merits stakeholder trust. A trustworthy system has provided validated evidence supporting claims about its behavior.

However, trustworthiness and trust are distinct: trustworthiness enables trust, but does not guarantee stakeholders will grant it. Conversely, stakeholders may grant trust to systems that subsequently fail to behave in trustworthy ways.


\section{Trust is Based on a System's Emergent Behavior}

How, then, do stakeholders decide to trust a system? Trust decisions are based on stakeholders' belief that the system's behavior will satisfy specific claims about performance, safety, or other requirements. Because system-level behavior emerges from complex interactions among components, trust must be evaluated at the system level specifically, at the boundary for which stakeholders are accountable.
It is essential to distinguish trust from trustworthiness: trust is a stakeholder decision, while trustworthiness is a system property. A system can be trustworthy (demonstrably meeting claims) without being trusted, and conversely, stakeholders may trust systems that are not fully trustworthy.


\begin{itemize}
    \item \textbf{Not Built-In}: Trust is not embedded in the system itself. A system can be designed to be trustworthy, but \textbf{trust is granted by stakeholders on the basis of their }interaction, interpretation, and assessment of a system's behavior against specific claims.
    \item \textbf{Context-Dependent}: Trust is nuanced and granted by stakeholders depending on the \textbf{environment}, \textbf{stakeholder expectations}, and their \textbf{prior experiences}.
    \item \textbf{Dynamic}: Trust is not constant. Stakeholders trust in the system evolves over time based on \textbf{feedback loops} positive experiences reinforce trust, while failures erode it.
    \item \textbf{Relational}: It arises from the\textbf{relationship} between the system and its stakeholders, not just from technical specifications.
\end{itemize}


%**********

\section{Trust Calibration: Technical Framework}


Trust calibration is the alignment between stakeholder trust and system trustworthiness within a given operational context~\cite{lee2004trust, latiff2024calibrating}. Lee and See (2004) established the foundational framework defining calibration as the correspondence between a person's trust in automation and the system's actual capabilities~\cite{lee2004trust}. Proper calibration means stakeholders appropriately trust systems that are trustworthy and appropriately withhold trust from systems that are not.

\subsection{The Calibration Problem: Overtrust and Undertrust}

Trust miscalibration manifests in two distinct failure modes, each imposing different operational risks:\\

\noindent\textbf{\underline{Overtrust (Misuse):}} Stakeholders trust a system beyond its demonstrated capabilities, leading to inappropriate reliance that can cause serious safety issues~\cite{latiff2024calibrating}. In human-AI collaboration contexts, over-trusting autonomous systems sometimes causes serious safety issues, particularly when system reliability fluctuates~\cite{okamura2020adaptive}.

Overtrust leads operators to:
\begin{itemize}
    \item Delegate tasks the system cannot reliably perform
    \item Reduce monitoring and vigilance below safe thresholds
    \item Fail to intervene when the system operates outside validated conditions
    \item Accept system outputs without appropriate verification
\end{itemize}

Among the most common tendencies associated with automation overtrust, or misuse, include complacency and automation bias~\cite{macdonald2023trusting} where operators uncritically defer to automated recommendations even when contradictory evidence exists.\\

\noindent\textbf{\underline{Undertrust (Disuse):}} Stakeholders withhold trust from systems that have demonstrated adequate trustworthiness, resulting in underutilization of capable systems~\cite{latiff2024calibrating, lee2004trust}. This leads to:
\begin{itemize}
    \item Manual execution of tasks the system could handle reliably
    \item Increased cognitive workload and operator fatigue
    \item Slower decision-making and reduced throughput
    \item Organizational resistance to beneficial automation
    \item Missed opportunities for performance enhancement
\end{itemize}

When errors occur, human operators tend to over-correct their trust levels and lower their expectations to a level below the capabilities of the system, thereby transitioning directly from overtrust to distrust~\cite{macdonald2023trusting}. This overcorrection demonstrates how calibration is not merely about initial trust establishment but requires continuous adjustment.\\

\noindent\textbf{\underline{Asymmetric Consequences:}} Both overtrust and undertrust are problematic overtrust may lead to misuse, while distrust may lead to disuse~\cite{lee2004trust}. The economic and safety costs differ systematically: overtrust creates reliability and safety risks through inappropriate delegation and inadequate monitoring, while undertrust creates efficiency and capability risks through missed automation benefits and excessive manual workload.

\subsection{Calibration Dimensions: Resolution and Specificity}

Lee and See (2004) introduced two additional dimensions beyond calibration that characterize appropriate trust: resolution and specificity~\cite{lee2004trust, berkeley2020calibrating}.

\begin{itemize}
    \item \emph{Resolution}: The match between variability in trust judgments and variability in system capabilities~\cite{latiff2024calibrating}. Good resolution occurs when stakeholders adjust their trust levels to match changes in system capabilities, for example, reducing trust when error rates increase~\cite{latiff2024calibrating}. Poor resolution occurs when trust remains static despite significant capability fluctuations, leading to persistent miscalibration. An operator with poor resolution might maintain constant trust in a system whose reliability varies from 95\% in normal conditions to 60\% in edge cases, a dangerous mismatch.
    
    \item \emph{Specificity}: The degree to which trust is associated with particular components, functions, or temporal periods of system operation~\cite{berkeley2020calibrating}. Functional specificity means trust is differentiated across system capabilities, while temporal specificity reflects sensitivity to changes in context and time that affect capability~\cite{berkeley2020calibrating}. High specificity enables nuanced trust decisions, trusting a system for routine operations while remaining skeptical during edge cases, or trusting the perception subsystem while questioning the decision logic.
\end{itemize}

Enhancing human-AI partnerships requires good calibration, high resolution, and high specificity of trust~\cite{lee2004trust, berkeley2020calibrating}. Systems achieving all three dimensions enable operators to grant appropriate trust that matches actual capabilities across varying conditions and subsystems.

\subsection{Factors Influencing Calibration}

Trust calibration is affected by the four inherent characteristics of trust established earlier:

\begin{itemize}
    \item \emph{Context-Dependence}: Calibration must account for the operational environment, task demands, and stakeholder expectations. A system may warrant trust in one context but not another, even with identical trustworthiness evidence. Degrees of familiarity and understanding of automated systems contribute to more accurate evaluation of system competence, promoting well-calibrated trust~\cite{liu2024developing}.
    
    \item \emph{Dynamic Nature}: Trust in automation is a dynamic process, continuously recalibrated with input of new information, knowledge, and experience~\cite{liu2024developing}. Calibration requires ongoing adjustment as systems evolve, stakeholders gain experience, and operational contexts shift. This dynamic quality means calibration is not a one-time achievement but a continuous process.
    
    \item \emph{Relational Basis}: Individual self-efficacy in using automation confidence in the ability to effectively utilize automation technologies plays a crucial role in shaping trust, with higher self-efficacy correlating with greater trust and willingness to use automated systems~\cite{liu2024developing}. Calibration depends on the quality of the stakeholder-system relationship, including transparency of behavior, clarity of claims, and accessibility of trustworthiness evidence.
    
    \item \emph{Not Built-In}: Because trust is granted rather than inherent, calibration requires deliberate design choices that help stakeholders accurately assess trustworthiness, such as interpretable outputs, uncertainty communication, and appropriate failure modes.
\end{itemize}

\section{Trust Calibration in High-Stakes Domains: Military and Medical Applications}

Having established the conceptual framework for calibration, overtrust, and undertrust, we now examine how these phenomena manifest in two safety-critical domains. Military aviation and medical diagnosis provide complementary perspectives: military applications emphasize real-time dynamic environments with catastrophic failure modes, while medical applications highlight sequential decision-making under diagnostic uncertainty. Together, these domains illuminate the full spectrum of calibration challenges in safety-critical AI systems, revealing both universal principles and domain-specific requirements.

\subsection{Domain Characteristics and Calibration Requirements}

AI technologies are being developed for military use within three broad categories that cut across tactical, operational, and strategic levels of warfare: algorithmic solutions for data integration and analysis, autonomous systems utilizing machine intelligence, and decision-support software that augments human decision-making~\cite{macdonald2023trusting}. Medical AI systems similarly span diagnostic support, treatment planning, and predictive analytics. Issues relating to proper trust calibration vary within each category, as do the implications of trust misalignment~\cite{macdonald2023trusting}.

\begin{itemize}
    \item \emph{Temporal Dynamics}: Military air combat operates on subsecond timescales where operators constantly evaluate and grant appropriate levels of trust to intelligent machines~\cite{macdonald2023trusting}, while medical diagnosis unfolds over minutes to hours, allowing more deliberative calibration. Both require dynamic adjustment as context shifts, but the time available for recalibration differs by orders of magnitude.
    
    \item \emph{Consequence Profiles}: Military miscalibration risks immediate lethal consequences and mission failure. Overtrust in air combat autonomy can result in aircraft loss, pilot death, or civilian casualties. Undertrust degrades combat effectiveness and may lead to mission failure. Medical miscalibration manifests as diagnostic errors affecting patient outcomes over longer horizons. Overtrust leads to acceptance of incorrect diagnoses, while undertrust results in rejection of accurate AI recommendations and diagnostic delays. Both impose severe costs, but with different temporal signatures and recovery opportunities.
    
    \item \emph{Verification Requirements}: In advance of formal verification methods for AI-based autonomy, the military domain pioneered new methods to train and test AI agent compliance with safety requirements, including flight envelope protection and aerial/ground collision avoidance, as well as ethical requirements including combat training rules, weapons engagement zones, and clear avenues of fire~\cite{defensescoop2024pentagon}. Medical systems similarly require validation against clinical standards, though with less emphasis on real-time physical safety constraints and more on statistical accuracy and clinical validity.
\end{itemize}

\subsection{Military Example: DARPA's Air Combat Evolution (ACE) Program}

\noindent\textbf{\underline{Program Overview and Objectives}}

The proliferation of AI in military operations will necessarily lead to more interaction between humans and intelligent machines, with operations increasingly relying on safe and effective human-machine teaming that depends on humans constantly evaluating and granting appropriate levels of trust known as trust calibration to intelligent machines~\cite{macdonald2023trusting}.

DARPA's Air Combat Evolution (ACE) program, begun in 2019, aims to develop trusted, scalable, human-level, AI-driven autonomy for air combat by using human-machine collaborative dogfighting as its challenge problem~\cite{darpa2023ace, darpa2021collaborative}. The program explicitly frames trust calibration as its central research focus, recognizing that technical capability alone is insufficient; pilots must appropriately calibrate their trust to leverage AI capabilities effectively.

In August 2020, the ACE program's AlphaDogfight Trials pitted eight teams whose AIs flew simulated F-16s in one-versus-one aerial dogfights, with the champion AI defeating an experienced F-16 fighter pilot 5-0 in simulated combat~\cite{darpa2021collaborative}. Subsequently, 21 test flights were conducted between December 2022 and September 2023, achieving the first-ever in-air tests of AI algorithms autonomously flying an F-16 against a human-piloted F-16 in within-visual-range combat scenarios~\cite{defensescoop2024pentagon}.\\

\noindent\textbf{\underline{Overtrust and Undertrust in Air Combat}}

The ACE program confronts both calibration failure modes directly. In air combat, overtrust creates acute safety hazards:
\begin{itemize}
    \item Inadequate monitoring: Pilots failing to track autonomous maneuvers during critical combat phases, assuming the AI will handle all contingencies
    \item Boundary violations: Failure to recognize when the AI operates outside validated conditions (e.g., novel adversary tactics, sensor degradation, unusual weather)
    \item Inappropriate delegation: Assigning life-or-death weapons employment decisions to systems not validated for autonomous lethal engagement
    \item Situational awareness degradation: Reduced attention to flight dynamics and tactical situation as pilots over-rely on AI recommendations
\end{itemize}

The consequences are catastrophic: aircraft loss, pilot death, fratricide, or civilian casualties. Among the most common tendencies associated with automation overtrust, or misuse, include complacency and automation bias~\cite{macdonald2023trusting} tendencies which prove lethal in air combat.

Combat undertrust equally degrades mission effectiveness:
\begin{itemize}
    \item Excessive cognitive workload: Manual execution of tasks the AI could handle reliably, increasing pilot fatigue and error rates
    \item Slowed decision-making: Time-critical tactical decisions delayed by human processing bottlenecks when AI recommendations are rejected
    \item Force structure limitations: Inability to leverage AI for force multiplication prevents one pilot from effectively managing multiple unmanned wingmen
    \item Mission degradation: Reduced combat effectiveness through human limitations rather than technical constraints
\end{itemize}

When errors occur, human operators tend to over-correct their trust levels and lower their expectations to a level below the capabilities of the system, thereby transitioning directly from overtrust to distrust~\cite{macdonald2023trusting}. In combat, this overcorrection following a single AI error can permanently degrade trust below the system's actual reliability, preventing effective human-machine teaming for the remainder of the mission or indefinitely.\\

\noindent\textbf{\underline{Trust Measurement and Calibration Methods}}

The ACE program implemented multi-modal trust assessment unprecedented in autonomous systems research:

\begin{enumerate}
    \item \emph{Physiological Monitoring}: Test pilots flew flights in an L-29 jet trainer at the University of Iowa Technology Institute's Operator Performance Laboratory, with the two-seat jet outfitted with sensors in the cockpit to measure pilot physiological responses, giving researchers clues as to whether the pilot is trusting the AI or not~\cite{darpa2021collaborative, uiowa2021opl}. This real-time physiological data, including heart rate variability, galvanic skin response, and respiration patterns, provides continuous, non-intrusive trust measurement during high-stress operations, revealing trust dynamics that pilots cannot self-report.
    
    \item \emph{Behavioral Metrics}: The program recorded time per disengagement, analogous to miles per disengagement used in self-driving cars, serving as a major calibration metric~\cite{darpa2021collaborative}. Frequent disengagements indicate undertrust or poor system performance; rare disengagements in challenging scenarios suggest either excellent performance or dangerous overtrust.
    
    Measurement techniques tracked where the evaluation pilot's head is pointing and where their eyes are looking around the cockpit, enabling researchers to see how much the pilot is checking on the autonomy by looking outside the window compared to time spent on battle management tasks~\cite{darpa2021collaborative}. This attentional allocation reveals trust levels through observed behavior: excessive monitoring indicates undertrust, while minimal monitoring may indicate overtrust or well-calibrated trust in a highly reliable system.
    
    \item \emph{Compliance-Based Trust Framework}: Program officials defined trust operationally: ``When we talk about trust in the ACE program, what we're talking about is compliance with well-codified norms. The AI agents are going to perform in some ways differently than humans. What we ultimately mean in trust is whether the system ultimately achieves the objectives of the dogfighting set while complying with those norms''~\cite{defenseone2024ai}.
    
    This grounds calibration in objective, verifiable criteria rather than subjective comfort, a critical distinction. Pilots may subjectively feel comfortable (overtrust) or uncomfortable (undertrust) regardless of actual system capability. Compliance-based trust focuses on whether the system demonstrably meets safety and ethical requirements, providing an objective calibration target.
\end{enumerate}

\noindent\textbf{\underline{Progressive Complexity for Calibration}}

The ACE program manager stated: ``Only after human pilots are confident that the AI algorithms are trustworthy in handling bounded, transparent and predictable behaviors will the aerial engagement scenarios increase in difficulty and realism''~\cite{idstch2020darpa}. This staged progression directly addresses calibration by building appropriate trust through graduated exposure:

\begin{itemize}
    \item \emph{Phase 1 - Simulation Validation (AlphaDogfight Trials)}: 
    \begin{itemize}
        \item Establishes baseline AI capabilities in controlled virtual environment
        \item Allows algorithm refinement without safety risks
        \item Provides initial exposure for pilot mental model formation
        \item Demonstrates basic competence before physical flight
    \end{itemize}
    
    \item \emph{Phase 2 - Human-Servo Flights}:
    \begin{itemize}
        \item Safety pilot manually executes AI-generated commands while the evaluation pilot observes
        \item Enables trust measurement without autonomy risk, the jet is not actually flown by an AI; rather a safety pilot in the front cockpit acts as a ``human servo actuator'' executing flight control inputs generated by an AI, so to the evaluator pilot in the backseat, it appears as if the AI is performing the aircraft maneuvers~\cite{darpa2021collaborative}
        \item Allows pilots to assess AI decision quality with human safety buffer
        \item Captures trust responses through physiological and behavioral metrics
    \end{itemize}
    
    \item \emph{Phase 3 - Live Autonomous Flight}:
    \begin{itemize}
        \item X-62A VISTA aircraft under full AI control with safety pilots monitoring~\cite{defensescoop2024pentagon}
        \item Validates simulation-to-reality transfer and reveals differences
        \item Demonstrates AI can comply with safety norms in physical flight
        \item Builds confidence through successful autonomous operations
    \end{itemize}
    
    \item \emph{Phase 4 - Combat Scenario Complexity}:
    \begin{itemize}
        \item Introducing multiple weapon options and aircraft to assess how AI agents handle clear avenues of fire restrictions set up to prevent fratricide, exceedingly important when operating with offensive weapons in a dynamic and confusing environment~\cite{darpa2021collaborative}
        \item Tests AI performance in increasingly realistic combat scenarios
        \item Evaluates both overtrust (inappropriate delegation) and undertrust (excessive intervention)
    \end{itemize}
\end{itemize}

DARPA officials recognized the importance of developing optimal curricula for interactions with AI systems early on to help calibrate the level of trust operators might have, noting that if early interactions with AI systems go well, operators might trend toward overtrusting them, while if initial interactions are poor, they may trend toward undertrusting~\cite{csis2024darpa}. This insight directly informed the ACE program's graduated exposure strategy. Early successes build foundational trust without inducing overtrust, while controlled exposure to AI limitations prevents excessive undertrust.\\

\noindent\textbf{\underline{Interface Design for Calibration}}

Transparency is one potential design feature for enhancing human-autonomy teaming, as engineering details relating to the machine interface can be influential to striking the proper balance between eliciting trust and encouraging overtrust~\cite{macdonald2023trusting}. The ACE program must carefully balance transparency requirements:

\begin{itemize}
    \item \emph{Transparency Benefits}:
    \begin{itemize}
        \item Enables pilots to understand AI decision rationale
        \item Supports appropriate mental model formation
        \item Allows recognition of out-of-bounds operation
        \item Facilitates appropriate trust calibration
    \end{itemize}
    
    \item \emph{Transparency Risks}: Natural language processing and synthetic speech enable conversational communication between humans and robots that improves transparency and trust, although anthropomorphizing can have negative effects, including encouraging overtrust in autonomous agents due to human-like speech patterns~\cite{macdonald2023trusting}. Human-like communication may induce inappropriate trust by suggesting human-like understanding and judgment that AI systems do not possess.
\end{itemize}

The ACE program addresses this tension through bounded transparency, communicating AI state, intended actions, and confidence levels without anthropomorphic presentation that might induce overtrust.

\subsection{Medical Example: AI-Assisted Diagnostic Systems}

\noindent\textbf{\underline{The Clinical Calibration Challenge}}

Despite the usefulness of AI-based diagnostic decision support systems, over-reliance by physicians on AI-generated diagnoses may lead to diagnostic errors, particularly among inexperienced doctors~\cite{tsuzuki2024facilitating}. This parallels military concerns about operator experience levels affecting calibration quality less experienced operators more susceptible to both overtrust and undertrust.

Overtrust in medical diagnosis manifests as:
\begin{itemize}
    \item Automation bias: Physicians accepting incorrect AI diagnoses without adequate verification
    \item Anchoring effects: Initial AI suggestion constraining physician's differential diagnosis consideration
    \item Reduced clinical reasoning: Diminished diagnostic skill development as physicians defer to AI
    \item Missed diagnoses: Failure to recognize when AI operates outside validated patient populations or conditions
\end{itemize}

Undertrust in medical diagnosis leads to:
\begin{itemize}
    \item Rejection of accurate recommendations: Physicians dismissing correct AI diagnoses due to insufficient confidence
    \item Diagnostic delays: Time wasted re-examining cases AI has already analyzed correctly
    \item System abandonment: Complete rejection of AI tools after initial errors, preventing benefit from actual capabilities
    \item Resource inefficiency: Redundant testing and analysis already performed computationally
\end{itemize}

Recent evidence indicates that biased AI decreased diagnostic precision of physicians, and providing explanations for AI reasoning did not improve diagnostic precision~\cite{tsuzuki2024facilitating}, a finding that challenges assumptions about transparency as a universal calibration solution, contrasting with military findings where transparency proved beneficial but required careful implementation to avoid overtrust from anthropomorphization.\\

\noindent\textbf{\underline{Empirical Calibration Study}}

A quasi-experimental study at Dokkyo Medical University in Japan investigated trust calibration interventions for AI-assisted diagnosis~\cite{tsuzuki2024facilitating}. The experimental design provides methodological insights applicable to military contexts:

\begin{itemize}
    \item Physicians allocated to intervention and control groups
    \item 20 clinical cases with AI-generated differential diagnosis lists (10 diagnoses per case)
    \item Intervention: physicians asked whether the final diagnosis appeared in the AI-generated list, which served as the trust calibration mechanism~\cite{tsuzuki2024facilitating}
    \item Outcome: diagnostic accuracy when physicians provided 1-3 possible diagnoses per case
\end{itemize}

Trust calibration did not significantly improve physicians' diagnostic accuracy in this formative study, potentially due to small sample size and suboptimal trust calibration methods~\cite{tsuzuki2024facilitating}. The results indicate that physicians' trust calibration of AI without objective indicators may lead to excessive confidence in AI, resulting in incorrect diagnostic decisions~\cite{tsuzuki2024facilitating}.

This negative result, where simple calibration intervention proved insufficient, illuminates critical requirements: \emph{simple awareness of AI reliability is inadequate for effective calibration}. The study revealed overtrust persisted despite the intervention, suggesting physicians required more robust feedback mechanisms, structured performance tracking, and objective calibration criteria principles directly paralleling the ACE program's multi-modal measurement approach.\\

\noindent\textbf{\underline{Advanced Medical Calibration Approaches}}

More sophisticated medical calibration mechanisms demonstrate convergence with military requirements:

\begin{enumerate}
    \item \emph{Dynamic Confidence Scoring}: Implementation of dynamic scoring frameworks integrating AI confidence scores, semantic similarity measures, and transparency weighting reduced override rates to 33.29\%, with high-confidence predictions (90-99\%) overridden at only 1.7\% and low-confidence predictions (70-79\%) at 99.3\%~\cite{alothman2025enhancing}.
    
    This demonstrates effective calibration through confidence-weighted recommendations; physicians appropriately trusted high-confidence predictions (low override rate indicating neither overtrust nor undertrust) while appropriately distrusting low-confidence predictions (high override rate indicating proper caution). This parallels the ACE program's emphasis on bounded, transparent behaviors where AI clearly signals its confidence and operating envelope.
    
    \item \emph{Adaptive Calibration Cues}: Methods that detect inappropriate calibration status by monitoring user reliance behavior and present trust calibration cues only when necessary to trigger recalibration~\cite{okamura2020adaptive} mirror the ACE program's attention-tracking approach. Unlike studies emphasizing continuous information display, adaptive presentation of simple cues significantly promoted trust calibration during over-trust situations~\cite{okamura2020adaptive}.
    
    This adaptive approach addresses a key challenge: continuous transparency can overwhelm operators or habituate them to warnings (reducing effectiveness). Adaptive cues presented only during detected miscalibration maintain salience and effectiveness applicable to both medical and military contexts.
    
    \item \emph{Transparency and Process Visibility}: Proposed solutions include showing physicians the reasoning process of AI diagnostic systems in advance or utilizing secondary AI systems that indicate when to rely on primary diagnostic support systems~\cite{tsuzuki2024facilitating} similar to military transparency requirements for tactical decisions, though the slower tempo allows more detailed explanation than combat timescales permit.
\end{enumerate}

\subsection{Comparative Analysis: Domain-Specific Calibration Requirements}

Table~\ref{tab:calibration_comparison} presents a comparative analysis of calibration requirements across military and medical domains.

\begin{table*}[htbp]
\centering
\caption{Comparative Analysis: Domain-Specific Calibration Requirements}
\label{tab:calibration_comparison}
\begin{tabular}{p{3cm}p{5cm}p{5cm}p{3cm}}
\hline
\textbf{Dimension} & \textbf{Military (ACE Program)} & \textbf{Medical (Diagnostic AI)} & \textbf{Common Principle} \\
\hline
Temporal Scale & Subsecond decisions during combat maneuvers & Minutes-to-hours diagnostic processes & Trust must match decision timescales \\
\hline
Overtrust Consequences & Aircraft loss, pilot death, civilian casualties, fratricide & Acceptance of incorrect diagnoses, inappropriate treatment, patient harm & Inappropriate reliance on flawed outputs \\
\hline
Undertrust Consequences & Excessive workload, slowed decisions, mission failure, inability to achieve force multiplication & Rejection of accurate diagnoses, diagnostic delays, reduced efficiency, system abandonment & Failure to leverage valid capabilities \\
\hline
Calibration Measurement & Physiological sensors, eye tracking, time-to-disengagement, compliance verification & Override rates, diagnostic accuracy, agreement with AI, clinical outcomes & Objective behavioral metrics essential \\
\hline
Trust Definition & Compliance with safety/ethical norms while achieving combat objectives & Appropriate reliance aligned with AI diagnostic accuracy and confidence & Capability-matched trust levels \\
\hline
Calibration Strategy & Progressive complexity, simulation-to-reality transition, multi-modal measurement, graduated exposure & Dynamic confidence scores~\cite{alothman2025enhancing}, adaptive cues~\cite{okamura2020adaptive}, override thresholds, structured feedback & Staged exposure with objective feedback \\
\hline
Transparency Role & Critical but must balance with overtrust risk from anthropomorphization~\cite{macdonald2023trusting} & Necessary but insufficient without objective performance indicators~\cite{tsuzuki2024facilitating} & Required but not solely sufficient \\
\hline
Resolution Requirements & Must track rapid capability variations across scenarios, weather, adversary tactics & Must track accuracy variations across patient populations, condition types, data quality & Dynamic adjustment to context \\
\hline
Specificity Requirements & Differentiate trust across maneuvers (takeoff vs. combat), subsystems (perception vs. tactics) & Differentiate trust across diagnostic tasks (imaging vs. lab interpretation), disease categories & Function-specific trust granularity \\
\hline
\end{tabular}
\end{table*}

\subsection{Synthesis: Universal Calibration Requirements for Safety-Critical AI}

Analysis across military and medical domains reveals converging technical requirements for trust calibration:

\begin{enumerate}
    \item \emph{Multi-Modal Measurement}: Real-time assessment of trust is critical for capturing dynamic changes, thereby facilitating trust calibration through continuous monitoring and adjustment~\cite{liu2024developing}. Both domains demonstrate that single-point assessments (e.g., post-task surveys) are inadequate for dynamic calibration. The ACE program's physiological monitoring and medical systems' override tracking exemplify this principle: trust cannot be inferred from self-report alone but requires objective behavioral observation.
    
    \item \emph{Objective Performance Criteria}: Trust must be operationalized as compliance with well-codified norms and achievement of objectives~\cite{defenseone2024ai}, whether combat rules or clinical standards. Subjective comfort is insufficient and potentially dangerous for high-stakes calibration. Operators may feel comfortable (indicating potential overtrust) or uncomfortable (indicating potential undertrust) regardless of actual system capability. Calibration requires grounding in measurable system performance against explicit requirements.
    
    \item \emph{Metacognitive Transparency}: Reports of metacognitive sensitivity on how correctly and incorrectly judgments are endorsed with appropriate confidence levels facilitate both trust calibration and optimal decision making in human-AI collaboration~\cite{lee2025metacognitive}. Both domains require systems that communicate their own uncertainty accurately. AI confidence scores must correlate with actual accuracy. High confidence should reliably predict correctness, low confidence should reliably predict potential errors. Miscalibrated AI confidence (overconfident or underconfident) prevents operators from calibrating their own trust appropriately.
    
    \item \emph{Progressive Validation}: Only after operators are confident that AI algorithms are trustworthy in handling bounded, transparent and predictable behaviors should scenarios increase in difficulty and realism~\cite{idstch2020darpa}. Both military and medical contexts benefit from graduated exposure rather than immediate deployment in complex scenarios. This staged approach builds appropriate trust incrementally while exposing operators to system limitations in controlled settings, preventing both extreme overtrust (from only seeing successes) and extreme undertrust (from premature exposure to failures).
    
    \item \emph{Context-Specific Calibration}: Trust calibration involves making AI systems both interpretable and uncertainty-aware to enable rapid understanding of system limitations and likely failures~\cite{okamura2020trust}. This is particularly critical where data may be limited and often of low quality, requiring rapid trust calibration so decision-makers can calibrate trust in system outputs appropriately~\cite{okamura2020trust}. Operators must understand not just that the system may fail, but when and why it is likely to fail, enabling appropriate trust reduction in high-risk contexts while maintaining appropriate trust in validated scenarios.
    
    \item \emph{Adaptive Mechanisms}: Existing techniques to measure trust discretely or disruptively are inadequate; continuous, non-intrusive measures are needed to support the process of trust calibration effectively~\cite{hergeth2016keeping}. Both the ACE program's physiological monitoring and medical systems' override tracking exemplify this principle. Calibration requires closed-loop feedback where the system monitors operator trust state, detects miscalibration, and provides corrective interventions, creating a dynamic equilibrium between operator trust and system trustworthiness.
    
    \item \emph{Overtrust and Undertrust Prevention}: Both domains require explicit mechanisms to prevent calibration failure modes:
    \begin{itemize}
        \item Overtrust prevention: Expose operators to system limitations and failure modes during training; implement forcing functions requiring explicit verification of critical decisions; provide transparency about uncertainty and operating boundaries
        \item Undertrust prevention: Demonstrate system reliability through progressive success; provide clear performance metrics showing capability; enable gradual delegation as trust builds appropriately; avoid overcorrection after isolated failures
    \end{itemize}
\end{enumerate}

The military applications, particularly the ACE program, provide the most comprehensive demonstration of these principles in practice. The X-62A team's demonstration that cutting-edge machine learning-based autonomy could be safely used to fly dynamic combat maneuvers while complying with American norms for safe and ethical use of autonomous technology~\cite{defensescoop2024pentagon} represents a landmark achievement in operationalizing trust calibration for safety-critical AI systems. The program successfully navigated the challenge of building appropriate trust, neither overtrust leading to dangerous complacency nor undertrust preventing effective human-machine teaming through multi-modal measurement, progressive validation, objective performance criteria, and careful interface design.

Medical applications complement this by revealing that transparency alone is insufficient, providing explanations for AI reasoning did not improve diagnostic precision~\cite{tsuzuki2024facilitating}, and that structured, objective calibration mechanisms with performance feedback are essential. This negative finding reinforces military insights about calibration complexity: operators require more than understanding to calibrate appropriately; they need objective evidence of system capability boundaries, continuous feedback on performance, and mechanisms to detect and correct miscalibration.

Together, these domains demonstrate that trust calibration is not merely a human factors challenge but a fundamental system engineering requirement for safety-critical AI. Achieving appropriate calibration, avoiding both overtrust and undertrust, requires deliberate design spanning system architecture, interface design, training protocols, operational procedures, and continuous monitoring. The ACE program's success in enabling pilots to appropriately trust AI for combat maneuvers while maintaining appropriate skepticism and monitoring provides an existence proof that well-calibrated human-AI teaming is achievable in even the most demanding environments.
%**********


\section{Trust Emerges Through Composition Across Abstraction Levels}

Trust in complex systems is not monolithic but emerges compositionally through interactions across multiple abstraction levels. Each level, from individual hardware components to integrated systems to operational deployments, involves distinct stakeholders, evidence types, and trust judgments. Critically, trust does not simply aggregate linearly across these levels; rather, \textbf{system-level trust emerges from complex interactions between levels, producing behaviors and trust requirements that cannot be predicted from examining any single level in isolation}.

\subsection{Abstraction Levels in System Trust}

Complex systems exhibit hierarchical structure across multiple abstraction levels, each characterized by distinct components, stakeholders, and trust requirements:

\begin{itemize}
    \item \emph{Component Level}: Individual hardware elements (processors, sensors, actuators) and software modules (algorithms, data structures, functions). Trust concerns component reliability, correctness, and specified behavior.
    
    \item \emph{Subsystem Level}: Integrated collections of components forming functional units (perception subsystem, control subsystem, communication subsystem). Trust concerns integration correctness, interface contracts, and subsystem-level properties.
    
    \item \emph{System Level}: Complete integrated system performing mission-level functions (autonomous aircraft, diagnostic AI, robotic platform). Trust concerns system-level emergent properties, end-to-end behavior, and mission capability.
    
    \item \emph{Operational Level}: System deployed in an operational context with human operators, environmental factors, and mission constraints. Trust concerns real-world performance, human-machine interaction, and adaptation to context.
\end{itemize}

Each level exhibits emergent properties that arise from component interactions but are not properties of individual components. Trust in these emergent properties constitutes a fundamental challenge in compositional trust.

\subsection{Trust Composition Mechanisms}

Trust does not propagate unchanged between abstraction levels. Instead, composition involves multiple mechanisms:\\

\noindent\textbf{\underline{Upward Trust Propagation}}

Component-level trust provides \emph{necessary but insufficient} foundation for system-level trust. High component trust enables but does not guarantee system trust:

\begin{itemize}
    \item \emph{Integration Trust}: Even when individual components are trusted, their integration may introduce unanticipated interactions, timing dependencies, or emergent failures. Trust in integration requires evidence beyond component-level testing.
    
    \item \emph{Interface Trust}: Components interact through interfaces with implicit assumptions. Trust requires that these assumptions hold across all operational scenarios, a property verifiable only at higher abstraction levels.
    
    \item \emph{Emergent Property Trust}: System-level properties (safety, liveness, fairness) emerge from component interactions. These properties require system-level verification and cannot be established through component testing alone.
\end{itemize}

\noindent\textbf{Military Example - F-35 Sensor Fusion}: Individual sensors (radar, electro-optical, infrared) undergo rigorous component-level testing and verification. Each sensor demonstrates high reliability and accuracy within specified parameters. However, trust in the fused sensor picture of the system-level emergent property requires additional validation:

\begin{itemize}
    \item Sensor fusion algorithms must correctly weight and integrate conflicting sensor data
    \item Timing synchronization between sensors must maintain coherence
    \item Failure modes where sensors provide contradictory information require graceful degradation
    \item Adversarial scenarios (jamming, spoofing) may affect sensors differently, requiring system-level resilience
\end{itemize}

Trust in the fused sensor picture emerges from but transcends component-level sensor trust. Operators must trust not just individual sensors but the emergent integrated awareness, a qualitatively different trust judgment requiring system-level evidence.\\

\noindent\textbf{\underline{Downward Trust Influence}}

System-level trust requirements constrain component-level trust needs. Operational trust judgments identify which component properties matter:

\begin{itemize}
    \item \emph{Critical Path Identification}: Not all components equally impact system trust. Operational experience reveals which components lie on critical paths for mission success, focusing component-level trust verification efforts.
    
    \item \emph{Failure Mode Prioritization}: System-level failure analysis identifies which component failure modes actually threaten mission success versus those with negligible system impact.
    
    \item \emph{Performance Requirement Refinement}: Operational trust requirements flow down to specify necessary component performance characteristics.
\end{itemize}

\noindent\textbf{Military Example - ACE Program Trust Requirements}: System-level requirement for pilot trust in autonomous combat maneuvers flows down to component-level requirements:

\begin{itemize}
    \item Perception system must provide confidence scores calibrated to actual accuracy
    \item Planning algorithms must generate trajectories within validated flight envelope
    \item Control systems must execute planned maneuvers within specified tolerances
    \item Safety monitor must detect and mitigate out-of-bounds behavior within reaction time constraints
\end{itemize}

These component requirements derive meaning only from system-level operational trust needs. Component verification without system-level context risks focusing on irrelevant properties while missing critical trust requirements.

\subsection{Emergent Behaviors and Trust}

Emergent behaviors are system properties arising from component interactions that individual components do not exhibit, posing fundamental challenges for compositional trust.\\

\noindent\textbf{\underline{Positive Emergence: Capabilities Beyond Components}}

Systems can exhibit trustworthy behavior exceeding component-level capabilities through beneficial emergence:

\begin{itemize}
    \item \emph{Graceful Degradation}: System maintains partial functionality despite component failures. Trust in system resilience emerges from architectural redundancy and adaptive reconfiguration properties that no single component possesses.
    
    \item \emph{Collective Intelligence}: Multiple simple components produce sophisticated system behavior. Trust in system-level intelligence emerges from component coordination, not individual component sophistication.
    
    \item \emph{Robust Performance}: System performs reliably across conditions where individual components show variability. Trust in system robustness emerges from the diversity and complementarity of components.
\end{itemize}

\noindent\textbf{Medical Example - Ensemble Diagnostic AI}: Individual diagnostic models may each achieve 85\% accuracy with different error patterns. An ensemble system combining five such models can achieve 95\% accuracy through majority voting and confidence weighting. Trust in the ensemble system's diagnostic capability emerges from the \emph{diversity} of component errors rather than individual component excellence. Stakeholders trust the system more than any individual component, positive emergence, enabling higher trust.

This positive emergence creates a trust composition challenge: validating emergent system capabilities requires system-level testing that exposes component interactions, not just component-level verification. Stakeholders must trust properties (ensemble accuracy) that no component individually exhibits.\\

\noindent\textbf{\underline{Negative Emergence: Failures from Component Interactions}}

Conversely, systems can exhibit untrustworthy behavior despite trusted components through adverse emergence:

\begin{itemize}
    \item \emph{Timing-Dependent Failures}: Components tested individually function correctly, but race conditions or timing dependencies cause system-level failures only under specific interaction sequences.
    
    \item \emph{State Space Explosion}: Each component operates in trustworthy states, but combined system state space includes unvalidated or unsafe configurations reachable only through component interactions.
    
    \item \emph{Resource Contention}: Components compete for shared resources (memory, bandwidth, computation), producing emergent performance degradation or failures not present in isolated component testing.
    
    \item \emph{Assumption Violations}: Components make implicit assumptions about operating context or other components. When assumptions violated through component interactions, emergent failures occur despite each component behaving correctly according to its specification.
\end{itemize}

\noindent\textbf{Aviation Example - Boeing 737 MAX MCAS}: The Maneuvering Characteristics Augmentation System (MCAS) accident sequence illustrates catastrophic negative emergence:

\begin{itemize}
    \item \emph{Component Level}: Angle-of-attack (AOA) sensor functioned according to specification (providing angle measurements). Flight control computer executed programmed logic correctly. Control surfaces responded to commanded inputs properly.
    
    \item \emph{Subsystem Level}: MCAS software implemented its algorithm as designed, adjusting trim based on AOA sensor input. Each subsystem (sensing, computation, actuation) performed specified functions.
    
    \item \emph{System Level}: Emergent failure arose from \emph{interaction between subsystems}: Single AOA sensor failure provided erroneous input; MCAS repeatedly commanded nose-down trim; pilots lacked adequate information about MCAS activation; cumulative trim commands exceeded pilot recovery capability. No individual component failed, yet system-level behavior proved catastrophic.
    
    \item \emph{Operational Level}: Operational context amplified emergent failure pilot training did not include MCAS failure scenarios; cockpit indicators did not clearly signal MCAS activation; time pressure prevented diagnosis and recovery.
\end{itemize}

This negative emergence destroyed trust despite component-level trustworthiness. Each component passed individual verification, yet system integration created unsafe emergent behavior. Stakeholders who trusted individual components correctly could not predict system-level untrustworthiness; the emergent property only became apparent through system-level interaction.

The 737 MAX case demonstrates that \textbf{compositional trust requires explicit attention to emergent properties that transcend component boundaries}. Trust verification must address not only ``does each component work correctly'' but ``do component interactions produce trustworthy system behavior.''

\subsection{Emergent Trust Properties in AI Systems}

AI systems introduce additional compositional trust challenges through non-deterministic behavior and continuous adaptation:\\

\noindent\textbf{\underline{Learning-Induced Emergence}}

Machine learning systems exhibit emergent behaviors that evolve over time:

\begin{itemize}
    \item \emph{Capability Emergence}: ML models develop capabilities not explicitly programmed through training data patterns. Trust must address whether emergent capabilities align with intended system behavior.
    
    \item \emph{Failure Mode Emergence}: Models develop unexpected failure modes on data distributions not represented in training. Emergent failures appear only at deployment, requiring ongoing trust recalibration.
    
    \item \emph{Interaction Dynamics}: Multiple ML components (perception, planning, control) trained independently exhibit emergent coupled dynamics when integrated. System behavior emerges from learned model interactions.
\end{itemize}

\noindent\textbf{Military Example - ACE Program Emergent Behaviors}: DARPA's Air Combat Evolution program encountered emergent AI behaviors requiring trust calibration:

\begin{itemize}
    \item \emph{Simulation-to-Reality Transfer}: AI algorithms trained in simulation exhibited emergent behaviors in physical flight not present in simulation. Component-level trust (algorithm performance in simulation) did not fully compose with system-level trust (algorithm performance in aircraft).
    
    \item \emph{Adversarial Adaptation}: AI agents developed emergent tactics not explicitly programmed, both desirable (novel combat maneuvers) and concerning (behaviors near safety boundaries). Trust calibration required evaluating whether emergent tactics complied with safety and ethical norms.
    
    \item \emph{Human-AI Interaction Dynamics}: Pilot trust emerged from interaction patterns between human and AI rather than AI capability alone. Emergent trust depended on AI transparency, predictability, and an appropriate confidence signaling system-level properties arising from component interactions.
\end{itemize}

The ACE program addressed emergent behavior through progressive validation: simulation validation established component-level algorithm trust; human-servo flights revealed human-AI interaction emergence; live autonomous flights exposed physical reality emergence; increasing scenario complexity surfaced combat-relevant emergent tactics. Each abstraction level revealed emergent behaviors invisible at lower levels, requiring iterative trust calibration.\\

\noindent\textbf{\underline{Compositional Verification Challenges for ML Systems}}

Traditional compositional verification approaches struggle with ML components:

\begin{itemize}
    \item \emph{Specification Gaps}: ML components often lack formal specifications, making compositional reasoning difficult. Trust in ML-containing systems requires new verification approaches addressing learned behavior composition.
    
    \item \emph{Non-Compositional Properties}: Neural network properties (adversarial robustness, fairness, calibration) do not compose robust components may yield non-robust systems. Trust verification must address system-level properties explicitly.
    
    \item \emph{Continuous Evolution}: Online learning systems continuously adapt, producing emergent behaviors post-deployment. Compositional trust must accommodate temporal evolution and ongoing recalibration.
\end{itemize}

\subsection{Stakeholder Trust Networks Across Abstraction Levels}

Different stakeholders operate at different abstraction levels, creating a network of trust dependencies:\\

\noindent\textbf{\underline{Component-Level Stakeholders}}

\begin{itemize}
    \item \emph{Hardware Engineers}: Trust based on component testing, stress testing, and environmental validation. Provide evidence of component reliability under specified conditions.
    
    \item \emph{Software Developers}: Trust based on unit testing, code reviews, static analysis. Provide evidence of module correctness for specified inputs.
    
    \item \emph{Algorithm Designers}: Trust based on mathematical analysis, algorithmic properties, complexity bounds. Provide evidence of algorithm correctness and performance.
    
    \item \emph{ML Engineers}: Trust based on training data quality, model validation, performance metrics. Provide evidence of learned model behavior on test distributions.
\end{itemize}

Component-level stakeholders provide trust evidence for individual elements but typically lack visibility into system-level emergent properties. Their trust judgments form the foundation for higher-level trust but cannot substitute for system-level validation.\\

\noindent\textbf{\underline{System-Level Stakeholders}}

\begin{itemize}
    \item \emph{System Architects}: Trust based on architectural analysis, interface specifications, integration testing. Responsible for ensuring component interactions produce trustworthy emergent behavior.
    
    \item \emph{Verification \& Validation Teams}: Trust based on system-level testing, fault injection, scenario validation. Provide evidence of system behavior under integrated operation.
    
    \item \emph{Safety Engineers}: Trust based on hazard analysis, failure mode analysis, safety case development. Focus on system-level emergent safety properties.
    
    \item \emph{Security Teams}: Trust based on penetration testing, attack surface analysis, security audits. Address emergent security vulnerabilities from component interactions.
\end{itemize}

System-level stakeholders must synthesize component-level trust evidence while validating emergent properties. Their trust judgments depend on component-level stakeholder evidence but require additional system-level verification to address emergence.\\

\noindent\textbf{\underline{Operational-Level Stakeholders}}

\begin{itemize}
    \item \emph{Operators/Pilots}: Trust based on operational experience, system predictability, failure recovery. Provide evidence of system behavior in real-world operational contexts.
    
    \item \emph{Mission Commanders}: Trust based on mission success rates, operational constraints, context adaptation. Evaluate whether system capabilities match mission requirements.
    
    \item \emph{Maintainers}: Trust based on diagnostic clarity, repair success, degradation patterns. Provide evidence of system reliability over operational lifetime.
    
    \item \emph{End Users}: Trust based on outcome quality, explanation adequacy, error handling. Ultimate arbiters of whether the system merits trust in practice.
\end{itemize}

Operational-level stakeholders experience emergent system behaviors invisible to component and system-level stakeholders. Their trust judgments reflect real-world emergence, including environmental factors, human interaction dynamics, and long-term adaptation patterns. Operational trust evidence flows back down to refine component and system requirements.\\

\noindent\textbf{\underline{Trust Dependency Networks}}

These stakeholders form trust dependency networks where higher-level trust depends on lower-level evidence:

\begin{itemize}
    \item \emph{Vertical Dependencies}: Operational stakeholders depend on system-level validation, which depends on component-level verification. Trust flows upward through evidence aggregation.
    
    \item \emph{Horizontal Dependencies}: Stakeholders at the same level depend on each other. System architects trust integration evidence from V\&V teams; V\&V teams trust requirements from architects. Trust requires coordination within abstraction levels.
    
    \item \emph{Feedback Dependencies}: Operational experience reveals emergent behaviors requiring component redesign. Trust evidence flows downward to refine lower-level requirements and verification.
\end{itemize}

\noindent\textbf{Military Example - F-35 Stakeholder Trust Network}:

\begin{itemize}
    \item \emph{Component Suppliers} (multiple nations, contractors): Provide component-level trust evidence through testing and certification
    \item \emph{Prime Contractor} (Lockheed Martin): Integrates components and provides system-level trust evidence through developmental testing
    \item \emph{Government Test Teams}: Independent verification of system-level properties and emergent behaviors
    \item \emph{Operational Test Units}: Provide operational-level trust evidence through realistic mission scenarios
    \item \emph{Operational Squadrons}: Accumulate long-term trust evidence through operational deployment
\end{itemize}

Each stakeholder level provides distinct evidence addressing different emergent properties. Operational pilots trust the aircraft based on accumulated operational experience (operational-level emergence). Their trust depends on but transcends government test evidence (system-level emergence), which depends on contractor integration testing (subsystem-level emergence), which depends on supplier component verification (component-level verification). Trust emerges compositionally through this stakeholder network.

\subsection{Design Principles for Trustworthy Composition}

To enable trustworthy compositional trust across abstraction levels:

\begin{enumerate}
    \item \emph{Explicit Emergence Management}:
    \begin{itemize}
        \item Identify potential emergent properties during architecture design
        \item Establish verification approaches for emergent behaviors
        \item Design for observable emergence enable monitoring of interaction-dependent properties
        \item Document assumptions about component interactions
    \end{itemize}
    
    \item \emph{Compositional Verification Strategy}:
    \begin{itemize}
        \item Component-level verification establishes behavioral contracts
        \item Integration testing validates interface assumptions
        \item System-level testing exposes emergent properties
        \item Operational testing validates real-world emergence
        \item Progressive validation addresses emergence at each abstraction level
    \end{itemize}
    
    \item \emph{Trust Boundary Definition}:
    \begin{itemize}
        \item Clearly delineate component, subsystem, system, operational boundaries
        \item Specify trust requirements at each boundary
        \item Establish evidence required to trust across boundaries
        \item Design for graceful degradation at trust boundaries
    \end{itemize}
    
    \item \emph{Stakeholder Trust Coordination}:
    \begin{itemize}
        \item Establish communication channels between abstraction-level stakeholders
        \item Enable operational feedback to system and component designers
        \item Coordinate trust evidence across stakeholder networks
        \item Maintain trust evidence provenance through system lifecycle
    \end{itemize}
    
    \item \emph{Continuous Trust Recalibration}:
    \begin{itemize}
        \item Monitor for emergent behaviors post-deployment
        \item Update trust based on operational experience
        \item Propagate trust updates across abstraction levels
        \item Maintain trust alignment as system and context evolve
    \end{itemize}
\end{enumerate}

\subsection{Synthesis: Compositional Trust as Emergent Phenomenon}

Trust in complex systems is itself an emergent phenomenon. Just as system behavior emerges from component interactions, system trust emerges from the composition of:

\begin{itemize}
    \item Component-level trust evidence across multiple components
    \item Integration trust addressing component interactions
    \item System-level trust validating emergent properties
    \item Operational trust reflecting real-world emergence
    \item Stakeholder trust judgments across organizational boundaries
\end{itemize}

\noindent\textbf{Trust does not reduce to component properties}. High component trust necessary but insufficient for system trust. Conversely, systems can merit trust despite component uncertainties through architectural resilience and graceful degradation.\\

\noindent\textbf{Emergence fundamentally challenges compositional trust}. Positive emergence enables capabilities exceeding component-level capabilities, requiring system-level validation of properties no component exhibits individually. Negative emergence produces failures despite trusted components, requiring explicit verification of interaction-dependent failure modes.\\

\noindent\textbf{Stakeholder networks propagate trust evidence}. Component-level stakeholders provide foundational evidence; system-level stakeholders validate integration and emergence; operational stakeholders provide real-world evidence. Trust emerges through evidence composition across this stakeholder network.\\

The military examples F-35 sensor fusion, ACE program AI behaviors, and 737 MAX integration failures demonstrate that compositional trust requires explicit attention to emergence at every abstraction level. Trust verification cannot stop at component boundaries but must address the emergent properties that make systems valuable (and potentially dangerous). Achieving trustworthy composition requires progressive validation strategies that surface emergence incrementally, enabling trust calibration before deployment in high-stakes operational contexts.

